.. Copyright © 2019–2024 Advanced Micro Devices, Inc

.. `Terms and Conditions <https://www.amd.com/en/corporate/copyright>`_.

=================================
Data Compression Library Tutorial
=================================

Data Compression and Hardware Acceleration
==========================================

Data Compression is reduction in the number of bits needed to represent the data. Compressing data saves storage capacity, speeds up data transfer, and decreases the cost to store data.

Why Acceleration is Required and How it Helps
=============================================

A general purpose CPU has its computation capabilities and limitations. Some additional hardware acceleration is used at times for performing some functions faster and more efficiently. Hardware accelerators improve the performance of a specific algorithm by allowing greater concurrency (i.e., parallel execution) based on the application. 

GZIP is one such algorithm which is widely used in applications, such as file storage, distributed systems, genetics, etc. Traditionally, the CPU based solutions are limited to Mb/s speed, but there is a high demand for an accelerated GZip which provides throughput in terms of Gb/s. Hence, you need to accelerate this algorithm. 

GZIP is combination of LZ77 and Huffman coding, and this is a block based processing algorithm. The main advantages of the block feature is that each block can be processed independently which enables greater concurrency and helps in achieving higher performance.

An **AMD Alveo™ card** can help improve performance in the following ways: 

(1) Instruction parallelism by creating a customized and long pipeline.
(2) Data parallelism by processing multiple blocks at the same time.
(3) Customizable memory hierarchy of BRAM/URAM/HBM, providing high bandwidth of memory access.

The AMD FPGA-based solution accelerates both compression and decompression with multicore and multibyte architectures thus speeding up the overall processing time which results in improved system throughput and efficient resource utilization.

How the Data Compression Library Works
=========================================

The data compression library is an open-sourced performance-optimized AMD Vitis™ library written in C++ for accelerating data compression applications on AMD Accelerator cards in a variety of use cases. The library covers two levels of acceleration: the module level and the pre-defined kernel level and evolve to offer the third level as pure software APIs working with pre-defined hardware overlays.

- L1: Module level, it provides optimized hardware implementation of the core LZ based and data compression algorithm specific modules. 
- L2: Kernel level, this section calls compression/decompression kernel which internally uses the optimized hardware modules to showcase various kernel demos.
- L3: The software API level will wrap the details of offloading acceleration with the prebuilt binary (overlay) and allows you to accelerate data compression tasks on Alveo cards without hardware development. It is designed as a specialized compression engine, multiple of which can run concurrently on the same AMD accelerator card to meet the high-throughput requirements of your algorithms. This reduces the bandwidth consumption and the overall infrastructure costs, on-premise or in the cloud.

The GZIP compression kernel takes the raw data as input and compresses the data in block based fashion and writes the output to global memory. LZ77 is a byte based compression scheme. The resulting output from this kernel is represented in a packet form of 32 bit length <Literal, Match Length, Distance>. It also generates output of literal and distance frequencies for dynamic Huffman tree generation. The output generated by this kernel is referred by the TreeGen and Huffman Kernel.

L3 API
~~~~~~

L3 API are more scalable solutions to achieve maximum performance with optimized host and kernel for an end-to-end solution. Here you are targeting a number of compute units to get the maximum throughput and get to know bandwidth saturation for the design.

This demo is aimed at showcasing the Alveo U250 acceleration of Gzip_app and Alveo U50 (HBM Platform) acceleration of Gzip_hbm for both compression and decompression; it also supports Zlib with a host argument switch.

.. code-block:: shell
   
   Tested Tool:  2022.1
   Tested XRT :  2022.1
   Tested XSA :  xilinx_u250_gen3x16_xdma_4_1_202210_1
   Tested XSA :  xilinx_u50_gen3x16_xdma_5_202210_1

+---------------------------------------------------------------------------------------------------------+--------------------------------+-------------------+----------+---------+-------+-------+--------+------------------------------------------------+
| Flow                                                                                                    |Target Compute Units            |Compression-Ratio  |  FMax    |  LUT    |  BRAM |  URAM | Memory | Througput                                      |
+=========================================================================================================+================================+===================+==========+=========+=======+=======+========+================================================+
|  Gzip_app                                                                                               |Compression 2, Decompression 8  |      2.70         |  300 MHz  |  202K   |  362  |  144  | DDR    |  Compression-632MBps , Decompression 408.8MBps |
+---------------------------------------------------------------------------------------------------------+--------------------------------+-------------------+----------+---------+-------+-------+--------+------------------------------------------------+
|  Gzip_hbm                                                                                               |Compression 6, Decompression 8  |      2.70         |  450 MHz  |  277K   |  503  |  208  | HBM    |  Compression-961MBps , Decompression 356MBps   |
+---------------------------------------------------------------------------------------------------------+--------------------------------+-------------------+----------+---------+-------+-------+--------+------------------------------------------------+

.. note:: Software Emulation not supported.

This application is present under the ``L3/demos`` directory. Follow the build instructions to generate the executable and binary.

The host executable generated is named "**xil_gzip**", and it is generated in the ``./build_dir.<TARGET mode>.<xsa_name>/`` directory.

Executable Usage
----------------

1. To execute a single file for compression:               
                                          ``./build_dir.<TARGET mode>.<xsa_name>/xil_gzip -xbin ./build_dir.<TARGET mode>.<xsa_name>/compress_decompress.xclbin -c <input file_name>``

2. To execute a single file for decompression:           :
                                            ``./build_dir.<TARGET mode>.<xsa_name>/xil_gzip -xbin ./build_dir.<TARGET mode>.<xsa_name>/compress_decompress.xclbin  -d <compressed file_name>``

3. To validate single file (compress & decompress): 
                                            ``./build_dir.<TARGET mode>.<xsa_name>/xil_gzip -xbin ./build_dir.<TARGET mode>.<xsa_name>/compress_decompress.xclbin -t <input file_name>``
4. To execute multiple files for compression:   
                                            ``./build_dir.<TARGET mode>.<xsa_name>/xil_gzip -xbin ./build_dir.<TARGET mode>.<xsa_name>/compress_decompress.xclbin -cfl <files.list>``
5. To execute multiple files for decompression:   
                                             ``./build_dir.<TARGET mode>.<xsa_name>/xil_gzip -xbin ./build_dir.<TARGET mode>.<xsa_name>/compress_decompress.xclbin -dfl <compressed files.list>``
6. To validate multiple files (compress & decompress): 
                                             ``./build_dir.<TARGET mode>.<xsa_name>/xil_gzip -xbin ./build_dir.<TARGET mode>.<xsa_name>/compress_decompress.xclbin -l <files.list>``

    - ``<files.list>``: Contains various file names with current path.

The default design flow is a GZIP design to run the ZLIB, enable the switch ``-zlib`` in the command line, as mentioned below: ``./build_dir.<TARGET mode>.<xsa_name>/xil_gzip -xbin ./build_dir.<TARGET mode>.<xsa_name>/compress_decompress.xclbin -c <input file_name> -zlib 1``.


L2 API
~~~~~~

L2 API are for users who has certain understanding of HLS and programming on FPGAs and want to make modification on kernels.

These APIs are more Vitis flow based designs in which communication and data transfer happens between kernel and host. Kernel works on data and output sent back to the host. The optimized kernel with best kernel performance that can be seen.  

By default, GZIP supports a 32 KB block size. But in your library, multiple block sizes are supported namely, 8 KB and 16 KB. Not only multiple block sizes but the data compression library has both dynamic and static Huffman modules which are optimized to give good performance. 

+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+-------------------+----------+---------+-------+-------+
| Architecture                                                                                                                        |  Compression Ratio   |     Throughput    |  FMax    |  LUT    |  BRAM |  URAM |
+=====================================================================================================================================+======================+===================+==========+=========+=======+=======+
|  GZipc 32KB  Compress Stream                                                                                                        |        2.70          |      2.0  Gb/s    |  300 MHz  |   54K   |  141  |  64  |
+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+-------------------+----------+---------+-------+-------+
|  GZip 8KB Compress Stream                                                                                                           |        2.70          |      2.0  Gb/s    |  300 MHz  |   57.5K |  100  |  48  |
+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+-------------------+----------+---------+-------+-------+
|  GZip 16KB Compress Stream                                                                                                          |        2.70          |      2.0  Gb/s    |  282 MHz  |   58K   |  164  |  48  |
+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+-------------------+----------+---------+-------+-------+
|  Gzipc_block_mm32KB                                                                                                                 |        2.70          |      2.0  Gb/s    |  300 MHz  |   57K   |  135  |  64  |
+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+-------------------+----------+---------+-------+-------+
|  Gzipc_static32KB                                                                                                                   |        2.70          |      2.0  Gb/s    |  300 MHz  |   35K   |  45   |  64  |
+-------------------------------------------------------------------------------------------------------------------------------------+----------------------+-------------------+----------+---------+-------+-------+

Library designs supports `Free-Running-Kernel <https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Free-Running-Kernel>`__ and `Memory-Mapped kernels <https://docs.xilinx.com/r/en-US/ug1393-vitis-application-acceleration/Memory-Mapped-Interfaces>`__.

``GZip/Zlib`` Memory Mapped and GZip/Zlib Compress Stream: Supports Dynamic Huffman.

``GZip/Zlib`` Streaming: Full standard support (Dynamic Huffman, Fixed Huffman and Stored Blocks supported)

Commands to Run L2 and L3 Cases
---------------------------------------

.. code-block:: shell

    cd L2/tests/    
    # build and run one of the following using U250 platform
    make run TARGET=hw_emu PLATFORM=/path/to/xilinx_u250_gen3x16_xdma_4_1_202210_1/
    
    # delete generated files
    make cleanall

Here, ``TARGET`` decides the FPGA binary type.

* ``hw_emu`` is for hardware emulation.
* ``hw`` is for deployment on physical card (compilation to hardware binary often takes hours).

Besides ``run``, the Vitis case makefile also allows ``host`` and ``xclbin`` as the build target.

L1 API 
~~~~~~

L1 API are for users who are familiar with HLS programming and want to tests/profile/modify the HLS modules. With the HLS test project provided in L1 layer, you could get:

(1) Function correctness tests, both in c-simulation and co-simulation.
(2) Performance profiling from HLS synthesis report and co-simulation.
(3) Resource and timing from Vivado synthesis.


Command to Run L1 Cases
-------------------------------

.. code-block:: shell

    cd L1/tests/
    
    make run CSIM=1 CSYNTH=0 COSIM=0 VIVADO_SYN=0 VIVADO_IMPL=0 \
        PLATFORM=/path/to/xilinx_u250_gen3x16_xdma_4_1_202210_1/

The test control variables are:

* ``CSIM`` for high level simulation.
* ``CSYNTH`` for high level synthesis to the register transfer level (RTL).
* ``COSIM`` for co-simulation between software test bench and generated RTL.
* ``VIVADO_SYN`` for synthesis by AMD Vivado™.
* ``VIVADO_IMPL`` for implementation by Vivado.

For all these variables, setting to ``1`` indicates execution while ``0`` for skipping. The default value of all these control variables are ``0``, so they can be omitted from command line if the corresponding step is not wanted.
